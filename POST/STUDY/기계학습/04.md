---
title: Lec4 Training Models
data: 2025-09-22
---

## 1. Linear Regression

선형 모델은 입력 특성들의 가중합에 bias 항을 더해 예측값을 계산한다.

### How to Train Linear Regression?

우선 모델이 훈련 데이터에 잘 맞도록 파라미터를 설정하는데, 이를 재는 척도로 RMSE(Root Mean Square Error, 평균제곱근오차) 를 사용한다. 따라서 선형 회귀 모델을 학습하려면, RMSE를 최소화하는 θ 값 (파라미터 가중치) 을 찾아야한다. 실제로는 MSE(Mean Squared Error, 평균제곱오차) 를 최소화하는 편이 간단하며 결과도 동일하다.

### Normal Equation

정규방정식은 closed-form 해, 즉 비용 함수를 최소화하는 θ 값을 직접 계산해 주는 수학적 식이다.

$$
\hat{\theta}=(X^\top X)^{-1}X^\top y
$$

최소제곱을 기반으로 계산하는 방법도 있다.

$$
\hat{\theta} = X^+ y
$$

이때 $X^+$ 는 X의 유사 역행렬로 특이값 분해 (SVD) 로 얻을 수 있다.

### Computational Complexity of Normal Equation

정규방정식은 $\mathbf{X}^\top\mathbf{X}$의 역행렬을 계산하는데, 이는 $(n+1)\times(n+1)$ 크기의 행렬이다 (n: 특성 개수)

역행렬로 만드는 계산 복잡도는 구현에 따라 보통 $\mathcal{O}(n^{2.4})$에서 $\mathcal{O}(n^{3})$ 수준으로 특성이 2배가 되면 계산 시간은 대략 8배 증가한다.

SVD 방식의 복잡도는 대략 $\mathcal{O}(n^{2})$ 으로 특성이 2배가 되면 4배 정도 증가한다. 

다시 말해 두 방식 모두 특성 수가 커질수록 매우 느려진다. 장점은 훈련 샘플 수 m 에 대해서는 선형 $\mathcal{O}(m)$ 이라, 메모리에 적재만 가능하다면 큰 훈련 세트도 효율적으로 처리할 수 있다.

---

## 2. Gradient Descent

특성이 매우 많거나 훈련 샘플이 너무 많아 메모리에 다 올릴 수 없는 경우에 더 적합한, 전혀 다른 방식을 알아보자.

Gradient Descent 는 비용 함수를 최소화하기 위해 파라미터를 반복적으로 조금씩 조정하는 방식으로, 오차 함수의 θ에 대한 gradient 를 측정하고, 내려가는 방향으로 이동하여 0이 되는 지점에 도달하면 최소값에 도달한 것이다.

우선 θ를 무작위 값으로 채우고, 아주 작은 보폭으로 값을 개선하며 비용 함수 (MSE) 가 줄도록 시도하는 것을 반복하여 최소값에 수렴할때까지 진행한다.

학습률 하이퍼파라미터: 이동 크기를 의미하며, 너무 작으면 매우 많은 반복이 필요하고, 너무 크면 알고리즘이 수렴하지 않을 수 있다.

모든 비용함수가 깔끔한 그릇 모양은 아니기 때문에, 무작위 초기화가 어디서 시작하냐에 따라 전역 최소값이 아닌 지역 최소에서 수렴할수도, 너무 오래 걸려서 최소값에 도달하지 못한채 끝날 수도 있다.

다행히 선형 회귀의 MSE 비용함수는 볼록 함수 (θ에 대해 2차식으로 정리됨) 로, 극값이 없고 기울기가 갑자기 튀지 않아 안정적으로 최소값에 접근함이 보장된다.

특성들의 스케일이 서로 크게 다르면 비용 함수가 길쭉하게 늘어난 그릇 모양이 될 수 있기 때문에, GD를 사용할 때는 모든 특성이 비슷한 스케일을 같도록 해야한다.

### Batch Gradient Descent

GD를 구현하기 위해서, 각 모델 파라미터에 대한 비용 함수의 기울기를 계산해야 한다 (파라미터를 아주 조금 바꿨을 때 비용 함수가 얼마나 변하는지). 편도함수(partial derivative) 를 구해야한다는 뜻이다.

그래디언트 벡터(gradient vector): 편도함수들을 모아 만든 벡터

$$
\nabla_\theta \mathrm{MSE}(\theta) =
\begin{pmatrix}
\frac{\partial}{\partial \theta_0}\mathrm{MSE}(\theta)\\
\frac{\partial}{\partial \theta_1}\mathrm{MSE}(\theta)\\
\vdots\\
\frac{\partial}{\partial \theta_n}\mathrm{MSE}(\theta)
\end{pmatrix} =
\frac{2}{m} X^\top (X\theta - y)
$$

매 스텝마다 전체 훈련셋 X 에 대해 계산을 수행해야 한다. 그래서 큰 훈련셋에서는 느리다. 하지만 특성 수가 많은 것에는 강하다.

파라미터 업데이트는 아래와 같이 진행한다.

$$
\theta_{\text{next}}=\theta-\eta,\nabla_\theta \text{MSE}(\theta)
$$

* $\eta$: **학습률(learning rate)**
* $nabla_\theta \text{MSE}=\frac{2}{m}X_b^\top(X_b\theta-y)$: **전체 데이터(배치 전부)**로 계산한 기울기

```python
eta = 0.1            # 학습률
n_iterations = 1000  # 반복 횟수
m = 100              # 샘플 수 (예시)

theta = np.random.randn(2,1)  # 무작위 초기화 (절편+기울기, 파라미터 2개)

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)  # 배치 기울기
    theta = theta - eta * gradients                  # 파라미터 한 스텝 업데이트
```

* `X_b`: 바이어스(절편) 항을 포함한 설계행렬(예: 첫 열이 1).
* `gradients`: 위 수식 (\frac{2}{m}X_b^\top(X_b\theta-y)) 구현.
* `theta` 갱신을 **1000번** 반복하면 최소값 근처로 수렴.

학습률이 너무 작은 경우 아주 오래 걸리고, 큰 경우 알고리즘이 발산하며 오히려 해에서 멀어진다.

적절한 학습률을 찾기 위해서 grid search를 사용할 수 있다.

iteration 수는 일단 아주 크게 설정하고, gradient가  아주 작은 값 $\epsilon$ 보다 작아질 때 중단하면 된다.

### Stochastic Gradient Descent

Batch GD는 매 스텝마다 전체 훈련셋으로 gradient를 계산하기 때문에 훈련셋이 크면 너무 느려진다.

Stochastic GD 는 매 스텝마다 무작위로 한 샘플을 뽑아, 그 샘플만으로 gradient를 계산한다.

당연히 훨씬 빠르지만, 무작위성으로 비용함수가 오르락내리락 한다. 그래도 장기적으로는 descent 한다.

알고리즘이 멈췄을 때의 최종 파라미터 값들은 좋긴 하지만 최적(optimal)은 아니다. 하지만 비용 함수가 매우 불규칙할 경우, 지역 최소값에서 빠져나올 수 있기 때문에 Batch GD 보다 전역 최소값을 찾을 가능성이 높다.

> 왜 자꾸 최소값을 찾냐? 오차가 최소가 되는 지점에서의 파라미터 가중치를 사용하려고!
> 그리고 이때 최소값이랑 아까 아주 작은 값 $\epsilon$은 또 다르다.
> 최소값을 찾으려고 기울기(gradient) 가 0인 지점을 찾고있는데, 0을 찾기는 어려우니까 아주 작은 값과 비교하는거임.

무작위성이 지역 최소 탈출에 도움을 주지만, 전역 최소에 안정적으로 머물지 못한다는 딜레마가 생긴다.

초반에는 큰 학습률로 시작하여 점차 줄이는 방식으로 진행하면 전역 최소에 안정적으로 정착할 수 있다.

이 또한 너무 빠르지도 너무 느리지도 않게 적절히 줄여가야한다.

m 회 반복하는 것을 한 회차로 여러 회차의 학습을 진행한다. 이때 한 회차을 epoch 이라 부른다.

1000회 반복했던 배치 GD와 달리, SGD는 50 epoch로 꽤 괜찮은 해를 구했다.

무작위로 샘플을 뽑기 때문에, 어떤 샘플은 여러번 뽑힐수도, 어떤 샘플은 뽑히지 않을수도 있다.

이를 방지하고 싶다면, 훈련셋을 섞은 후 차례로 골라가고, 다시 섞고, 하는 방식으로 진행하면 된다.

혹시 샘플이 정렬되어있다면, SGD는 한 라벨에 맞춰 먼저 최적화하고 다음 라벨로 넘어가고 해서 전역 최소에 못 도달할 수 있다.

### Mini-batch Gradient Descent

Mini-batch GD 는 미니배치라고 부르는 작은 무작위 샘플 묶음에 대해 gradient를 계산한다. GPU를 사용할때 행렬 연산의 하드웨어 최적화로 성능 향상을 얻을 수 있다는 장점이 있다. 그리고 SGD 보다 진행이 덜 불규칙적이다.

실제 최소값에서 멈추는 배치 GD와 달리, SGD와 mini-batch GD 는 최소값 근처를 계속 돌아다닌다.

### Comparison of Algorithms for Linear Regression

여기 슬라이드 내용을 한국어 **MD 표**로 정리했어요.

| Algorithm       | Large **m** (many samples) | Out-of-core support | Large **n** (many features) | Hyperparams | Scaling required | Scikit-Learn       |
| --------------- | -------------------------- | ------------------- | --------------------------- | ------------- | ---------------- | ------------------ |
| Normal Equation | Fast                       | No                  | Slow                        | 0             | No               | N/A                |
| SVD             | Fast                       | No                  | Slow                        | 0             | No               | `LinearRegression` |
| Batch GD        | Slow                       | No                  | Fast                        | 2             | Yes              | `SGDRegressor`     |
| Stochastic GD   | Fast                       | Yes                 | Fast                        | ≥2            | Yes              | `SGDRegressor`     |
| Mini-batch GD   | Fast                       | Yes                 | Fast                        | ≥2            | Yes              | `SGDRegressor`     |

* n: 특성(feature) 개수
* m: 데이터 샘플 개수

---

## 3. Polynomial Regression

데이터가 선형보다 복잡다면? 선형 모델로 비선형 데이터도 fit 할 수 있다. 각 특성의 거듭제곱 항들을 새 특성으로 추가하고, 이 확장 특성 집합을 선형 회귀 하는 것이다. 이것을 Polynomial Regression 이라고 한다. 

특성이 여러개인 경우 Polynominal Regression은 특성들 사이의 관계도 찾아낼 수 있다. 특성이 a, b 두 개이고 `degree=3`이면, $a^2, a^3, b^2, b^3$뿐 아니라 교차항 $ab, a^2b, ab^2$도 추가되기 때문이다.

`PolynomialFeatures(degree=d)`는 특성 n개짜리 배열을 $\binom{n+d}{d}=\frac{(n+d)!}{d!,n!}$ 개의 특성을 갖는 배열로 변환한다. 특성 수 폭증 주의!

---

## 4. Learning Curves

### How Can We Decide How Complex a Model Should Be?

선형 회귀는 과소적합할 수 있고, 고차 다항 회귀는 심각한 과적합을 일으킬 수 있다. 이때 복잡도를 어떻게 정할 것인가? → 교차검증을 사용해 모델의 일반화 성능을 추정하는 방법도 있고, learning curves를 살펴보는 방법도 있다.

훈련셋의 서로 다른 크기의 부분 집합들로 모델을 여러번 학습시켜 결과를 그리는 것이다.

선형 회귀 모델의 경우, 훈련 샘플이 작은 지점에서는 오차는 작고, 검증 오차가 크다. 훈련 샘플이 큰 지점으로 갈수록 두 곡선이 매우 가까운 수준에서 평탄화에 도달한다. 그리고 값도 꽤 크다. 전형적인 과소적합의 모습이다.

10차 다항 회귀 모델의 경우, 훈련 데이터의 오차가 선형 회귀 모델보다 낮고, 검증 오차와의 간격이 크다. 과적합의 전형적인 형태이다. 더 큰 훈련 세트를 사용한다면 두 곡선은 계속 서로 가까워질 것이다.

---

## 5. Regularized Linear Models

### Regularization

overfit을 줄이는 방법은 모델을 정규화하여 자유도를 줄이는 것이다. 다항 모델은 차수를 낮추고, 선형 모델은 파라미터 가중치에 제약을 거는 방식으로 정규화한다.

### Ridge Regression

비용 함수에 정규화 항 $\alpha \sum_{i=1}^{n} \theta_i^2$을 추가하여, 가중치를 가능한 작게 유지하도록 강제한다.

하이퍼 파라미터 α는 정규화 강도를 조절한다: 0이면 일반 선형 회귀와 동일하고, 매우 크면 모든 가중치가 0에 가까워져 그냥 평평한 직선이 된다

ridge 회귀는 입력 특성의 스케일에 민감하여 데이터 스케일링을 하는 것이 중요하다. 

### Lasso Regression

일부 가중치를 0으로 만들어 중요도가 낮은 특성의 가중치를 제거한다.

### Elastic Net

ridge와 lasso의 중간 지점이다. 두 방법을 단순 혼합한 형태로, 혼합 비율 r을 조절할 수 있다.

$$
J(\theta)=\mathrm{MSE}(\theta)+\alpha r\sum_{i=1}^{n}|\theta_i|+\frac{1-r}{2}\alpha \sum_{i=1}^{n}\theta_i^{2}
$$


```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])   # -> array([1.5433...])
```

### Early Stopping

검증 오차가 최소에 도달하면 멈추는 방식으로도 과적합을 방지할 수 있다.

---

## 6. Logistic Regression

### Estimating Probabilities

어떤 샘플이 특정 클래스에 속할 확률을 추정하는데 사용한다.

로지스틱 회귀가 예측하는 법

1. 점수 계산: 입력 x 에 가중치·바이어스를 곱해 점수 z 를 만든다.

   $z = \theta_0 + \theta_1 x_1 + \cdots + \theta_n x_n$

2. 확률로 변환: 점수 z를 시그모이드(로지스틱) 함수에 넣어 확률 $\hat p$ 로 바꾼다.

   $\hat p = \sigma(z) = \dfrac{1}{1+e^{-z}}$ → 0~1 사이의 값

3. 클래스 결정: 확률 $\hat p$가 기준(보통 0.5) 이상이면 1(양성), 아니면 0(음성) 로 예측한다.
   
$$
\hat y= \begin{cases} 
1 & \text{if } \hat p \ge 0.5 \\ 
0 & \text{if } \hat p < 0.5 \end{cases}
$$

### Training and Cost Function

#### 무엇을 하려는 건가?

* 목표: **양성(y=1)** 은 **높은 확률**로, **음성(y=0)** 은 **낮은 확률**로 예측하게 파라미터를 맞추기.
* 그래서 **맞게 예측할수록 작은 값**, **틀리게·확신할수록 큰 값**을 주는 벌점이 필요해요.

#### 벌점(코스트)을 이렇게 줍니다

* 샘플이 **양성(y=1)** 이면 비용:  $c=-\log(\hat p)$
  (여기서 $\hat p$ = 모델이 “양성일 확률”이라고 낸 값)
* 샘플이 **음성(y=0)** 이면 비용: $c=-\log(1-\hat p)$

왜 로그? $-\log(t)$는

* $t\to 1$ 이면 **0에 가까움** → 잘 맞춘 확률엔 **작은 벌점**
* $t\to 0$ 이면 **엄청 커짐** → 틀렸는데 **확신**한 예측엔 **큰 벌점**

#### 숫자로 직감 잡기

**양성 샘플(y=1)** 일 때:

* $\hat p=0.90$ → 비용 $-\log(0.90)\approx 0.105$ (거의 0, 잘했다!)
* $\hat p=0.50$ → $\approx 0.693$ (애매)
* $\hat p=0.01$ → $\approx 4.605$ (최악: 양성인데 1%라고 확신)

**음성 샘플(y=0)** 일 때는 $\hat p$ 대신 $1-\hat p$에 위 로직을 그대로 적용.

#### 한 줄 요약

* **맞을수록 0에 가깝게, 틀릴수록(특히 확신하고 틀리면) 크게** 벌점 주는 함수가
  $y=1\Rightarrow -\log(\hat p)$, $y=0\Rightarrow -\log(1-\hat p)$ 입니다.
* 이렇게 모든 샘플의 비용을 **평균**해서(=교차엔트로피) **그 값을 최소화**하도록 파라미터를 배우는 게 로지스틱 회귀 학습이에요.

#### 1) 전체 비용함수(= 로그 손실, 크로스 엔트로피)

모든 샘플의 비용을 **평균**한 게 최종 비용:
$$
J(\theta)= -\frac{1}{m}\sum_{i=1}^{m}\Big[y^{(i)}\log\hat p^{(i)} + (1-y^{(i)})\log\big(1-\hat p^{(i)}\big)\Big]
$$

* $\hat p^{(i)}=\sigma(\theta^\top x^{(i)})$: i번째 샘플을 양성으로 볼 **추정 확률**
* 맞게 예측하면 항이 작아지고(0에 가까움),
  틀리게 “확신”하면 항이 커져 **강한 벌점**

#### 2) 나쁜 소식: 닫힌형 해(정규방정식)가 없음

* 선형회귀처럼 $\hat\theta$를 한 방에 구하는 **공식(폐형 해)**이 **알려져 있지 않다** → 보통 **최적화(경사하강법 등)**로 푼다.


#### 3) 좋은 소식: 비용함수가 **볼록**

* 이 $J(\theta)$는 **볼록(convex)** → 학습률만 너무 크지 않고 충분히 돌리면, **GD 같은 최적화가 전역 최소**로 간다(지역 최소 걱정 X).


#### 4) 그래디언트(부분 미분) 공식

$$
\frac{\partial J}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m}\Big(\sigma(\theta^\top x^{(i)})-y^{(i)}\Big),x^{(i)}_j
$$

* 즉, **(예측확률 − 정답)** 에 **특성값**을 곱해 평균낸 것.
* 이걸 이용해

$$
\theta \leftarrow \theta - \eta \nabla_\theta J(\theta)
$$

로 업데이트하면 됩니다.

### 한 줄 요약

* 로지스틱 회귀는 **로그손실의 평균**을 **최소화**하도록 $\theta$를 찾는다.
* **공식 해는 없다** → **경사하강법**으로 푼다.
* 다행히 **비용은 볼록**이라 잘만 돌리면 **전역 최소**에 도달!


### Decision Boundaries

Iris 데이터셋: 세 가지 붓꽃 종(Iris setosa, Iris versicolor, Iris virginica)의 꽃받침(sepal)과 꽃잎(petal)의 길이·너비 측정값이 담긴, 150개 샘플로 이루어진 유명한 데이터셋이다. 꽃잎 너비만으로 Iris virginica를 분류하는 분류기를 만들어보자.

꽃잎 너비가 0cm에서 3cm까지 변할 때, 모델이 추정한 품종별 확률을 살펴보자.

Iris virginica(삼각형 표식)의 꽃잎 너비는 1.4cm ~ 2.5cm 범위에 있고,
다른 두 종(사각형 표식)은 대체로 더 작아서 0.1cm ~ 1.8cm 범위에 있다.
(약간의 겹침이 있다는 점에 주의.)

약 2cm 이상에서는 분류기가 그 꽃이 Iris virginica일 확률을 매우 높게 내며(즉, 강한 확신), 1cm 이하에서는 Iris virginica가 아닐 확률을 매우 높게 냅니다.

그 중간 구간에서는 분류기가 확신이 낮습니다. 하지만 predict_proba() 대신 **predict()**로 클래스를 직접 예측시키면, 가장 확률이 높은 클래스를 반환합니다.

따라서 두 확률이 각 50%로 같아지는 지점, 대략 1.6cm 부근에 **결정 경계(decision boundary)**가 생깁니다.

두 개의 특성 (꽃잎 너비와 길이)로 분류기를 만든다면 확률 50%로 추정하는 지점이 점이 아닌 선으로 나타난다.

다른 선형 모델들과 마찬가지로, 로지스틱 회귀도 ℓ1 또는 ℓ2 패널티로 정규화할 수 있습니다.


### Softmax Regression

소프트맥스 회귀(= 다항 로지스틱 회귀)

**다중 클래스**(예: 0/1/2/…/K−1)를 **한 번에** 예측하고, 각 클래스의 **확률 분포**까지 얻고 싶을 때 사용한다.

#### 클래스별 점수 계산

$$
s_k(\mathbf{x})=\theta^{(k)\top}\mathbf{x}\quad (k=1,\dots,K)
$$

각 클래스마다 **자기 파라미터 벡터** $\theta^{(k)}$를 가짐.
이들을 행으로 쌓으면 **파라미터 행렬** $\Theta\in\mathbb{R}^{K\times d}$

#### 소프트맥스로 확률화

$$
\hat p(y=k\mid \mathbf{x})=\frac{\exp\big(s_k(\mathbf{x})\big)}{\sum_{j=1}^{K}\exp\big(s_j(\mathbf{x})\big)}
$$

모든 클래스 확률의 **합이 1**이 되도록 정규화(그래서 “normalized exponential”).

#### 예측 클래스

$$
\hat y=\arg\max_k\ \hat p(y=k\mid \mathbf{x})
$$

#### 학습(훈련)

* **목표**: 정답 라벨 (y)에 대한 **교차엔트로피 손실**을 **최소화**:

$$
J(\Theta)= -\frac{1}{m}\sum_{i=1}^{m}\log\hat p\big(y^{(i)}\mid \mathbf{x}^{(i)}\big)
$$

* **최적화**: 경사하강법(미니배치/Adam 등)으로 $\Theta$ 업데이트.
* **정규화**: $\ell_2$ (기본), $\ell_1$, 또는 엘라스틱넷 가능 → 과적합 방지.

#### 로지스틱(이진)과 차이

* **로지스틱**: 두 클래스 중 **하나**의 확률만 모델링 $K=2$.
* **소프트맥스**: **K개 모든 클래스**의 확률을 **동시에** 모델링(한 번의 모델로 끝).

#### 포인트

* 점수 $s_k$가 큰 클래스일수록 $\exp(s_k)$도 커져 **확률↑**.
* 모든 확률은 서로 경쟁(정규화)하므로, **한 클래스가 오르면 다른 클래스는 상대적으로 내려감**.



#### 뭐 하는 모델?

* 클래스가 여러 개인 문제(예: 고양이/개/토끼)에서, **각 클래스일 확률**을 한 번에 계산한다.
  
각 클래스 k마다 점수 $s_k(\mathbf{x})=\big(\theta^{(k)}\big)^\top \mathbf{x}$ 를 만든다.

$\theta^{(k)}$: 클래스 k 전용 가중치 벡터

점수를 확률로 바꾼다 (소프틑 맥스)

$$
\hat p_k=\frac{e^{,s_k(\mathbf{x})}}{\sum_{j=1}^{K} e^{,s_j(\mathbf{x})}}
$$

* 지수 $e^{(\cdot)}$를 씌운 뒤 **전체 합으로 나눠 정규화** → **모든 확률 합이 1**이 됨.
* 점수가 큰 클래스일수록 확률이 커짐.

예측 클래스 뽑기: 확률이 가장 큰 클래스를 선택!

$$
\hat y=\arg\max_k \hat p_k
$$

요약: **점수 만들고 → 소프트맥스로 확률 만들고 → 제일 큰 확률의 클래스를 고른다.** 끝! 🎯

정답 클래스(타깃 클래스)의 **확률을 높게**, 나머지 클래스들의 확률은 **낮게** 예측하도록 파라미터를 학습시켜야한다.

#### 비용함수: 크로스 엔트로피(cross-entropy)

$$
J(\Theta)= -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K} y_k^{(i)} \log \hat p_k^{(i)}
$$

* $\hat p_k^{(i)}$: 샘플 i가 **클래스 k**일 **예측 확률**(소프트맥스 출력)
* $y_k^{(i)}$: 샘플 i의 **정답 원-핫 벡터**. 정답이 k면 1, 아니면 0.
* 의미: 정답 클래스의 예측 확률이 **낮으면 큰 벌점**, **높으면 작은 벌점(0에 가까움)** → 이 값을 **최소화**하면 모델이 정답 클래스 확률을 올리게 됨.

#### 왜 이게 좋은가?

* 정답 클래스에 **낮은 확률**을 주면 $-\log(\hat p)$가 **크게** 늘어 페널티 ↑
* 정답 클래스에 **높은 확률**을 주면 $-\log(\hat p)$가 **0에 가깝게** 줄어 페널티 ↓

#### 이진 분류와의 관계

* 클래스가 **두 개(K=2)**이면, 이 **크로스 엔트로피**는 **로지스틱 회귀의 로그 손실**과 **완전히 동일**해요.

요약: **소프트맥스 회귀는 “정답 클래스의 로그확률을 최대화(= 크로스엔트로피 최소화)”하도록 학습**하는 다중분류 모델입니다.



비용함수의 각 클래스 $\theta^{(k)}$ 에 대한 그래디언트는 아래와 같다.

$$
\nabla_{\theta^{(k)}} J(\Theta)= \frac{1}{m}\sum_{i=1}^{m}\big(\hat p_k^{(i)}-y_k^{(i)}\big),x^{(i)}
$$

이제 **각 클래스의 그래디언트**를 계산할 수 있으므로, **gradient descent** 로 비용을 최소화하는 파라미터 행렬 $\Theta$ 를 찾을 수 있다.

**소프트맥스 회귀로 아이리스(Iris) 세 품종 모두를 분류하기**

  ```python
  X = iris["data"][:, (2, 3)]  # petal length, petal width
  y = iris["target"]

  softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)
  softmax_reg.fit(X, y)
  ```

  * `multi_class="multinomial"`: 소프트맥스(다항 로지스틱)로 전환
  * `C=10`: **L2 정규화의 하이퍼파라미터**(작을수록 규제가 강함)


**꽃잎 길이 5cm, 꽃잎 너비 2cm**인 표본을 예측:

  ```python
  softmax_reg.predict([[5, 2]])        # -> array([2])
  softmax_reg.predict_proba([[5, 2]])
  # -> array([[6.38e-06, 7.43e-01, 2.57e-01]])  (예시)
  ```

  * 답: **Iris virginica (클래스 2)** 로 **약 94.2% 확률**.

**소프트맥스 회귀의 결정 경계**

* 배경 색 영역은 각 클래스가 **가장 높은 확률**을 가지는 구간을 보여줌.
* 등고선은 그 클래스의 **예측 확률 수준**을 표시.








