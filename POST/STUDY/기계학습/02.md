---
title: Lec2 End-to-End Machine Learning Project
date: 2025-09-08
---

### Working with Real Data

* Popular open data repositories

  * [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/)
  * [Kaggle datasets](https://www.kaggle.com/datasets)
  * [Amazon's AWS datasets](https://docs.aws.amazon.com/data-exchange/)

* Meta portals (they list open data repositories)

  * [Data Portals](http://dataportals.org/)
  * [OpenDataMonitor](https://opendatamonitor.eu/)
  * [Quandl](https://data.nasdaq.com/)
  * [AI Hub](https://aihub.or.kr/)
 
* Other pages listing many popular open data repositories
  
  * [Wikipedia's list of Machine Learning datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research)
  * [Quora.com](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)
  * [The datasets subreddit](https://www.reddit.com/r/datasets/)

* 이번 장에서는 StatLib 저장소의 **California Housing Prices** 데이터셋을 사용합니다.

  * 이 데이터셋은 **1990년 캘리포니아 인구조사** 자료를 기반으로 합니다.
  * 캘리포니아의 각 **블록 그룹(= 행정 구역)** 에 대해 **인구, 가구 소득 중앙값, 주택 가격 중앙값** 등의 지표가 포함됩니다.
  * 우리 모델은 이 데이터를 학습하여, **다른 모든 지표가 주어졌을 때 어떤 구역의 주택 가격 중앙값을 예측**할 수 있어야 합니다.
 
---

## 1. Look at the Big Picture

### Frame the Problem

* 모델을 만드는 것 자체가 최종 목표는 아닐 가능성이 큽니다.

* 회사는 이 모델을 어떻게 활용하고 어떤 이익을 기대하나요?

  * 목표를 아는 것은 중요합니다. 목표에 따라 **문제를 어떻게 정의할지**, **어떤 알고리즘을 선택할지**, **모델을 평가할 성능 지표가 무엇인지**, **미세 조정에 어느 정도 노력을 들일지**가 결정되기 때문입니다.

* 당신의 목표

  * 당신의 모델 출력(각 구역의 **주택 가격 중앙값** 예측)은 여러 다른 신호와 함께 **다른 머신러닝 시스템**으로 전달됩니다.
  * 그 **후속 시스템**은 특정 지역에 **투자할 가치가 있는지** 여부를 판단합니다.
  * 이는 **매출에 직접적인 영향을 주므로**, 정확히 수행하는 것이 매우 중요합니다.

* 부동산 투자를 위한 머신러닝 파이프라인

  **Upstream components** → 
  
  **District data(구역 데이터)** 저장소 → 
  
  **District pricing(구역 가격 산정)** *(Your component: 당신의 구성요소)* → 
  
  **District prices(구역별 가격)** 저장소 → 
  
  **Investment analysis(투자 분석)** *(Other signals: 다른 신호와 함께 입력)* → 
  
  **Investments(투자 내역)** 저장소

* 파이프라인(Pipelines)

  * 일련의 데이터 처리 구성요소들의 흐름을 **데이터 파이프라인(data pipeline)** 이라고 합니다.
  * 각 구성요소는 보통 **비동기적으로** 동작합니다.

* 다음으로 확인해야 할 것은 **현재 해결 방식이 어떤지**입니다.

  * 지금은 전문가들이 **수작업으로 구역별 주택 가격을 추정**합니다. 팀이 해당 구역의 최신 정보를 수집하고, **주택 가격 중앙값을 직접 얻지 못하면 복잡한 규칙으로 추정**합니다.
  * 이 방식은 **비용과 시간이 많이 들고** 정확도도 높지 않습니다. 실제 중앙값을 확인해 보면 **추정치가 20% 이상 벗어나는 경우**가 자주 있습니다.

* 이제 이 모든 정보를 바탕으로 **시스템 설계를 시작**할 수 있습니다.

  * 먼저 문제를 규정하세요: **지도학습, 비지도학습, 강화학습** 중 무엇인가?
  * **분류 문제인지, 회귀 문제인지**, 혹은 다른 유형인지?
  * **배치 학습**을 쓸지, **온라인 학습** 기법을 쓸지?

* **지도학습, 비지도학습, 강화학습 중 무엇인가?**

  * 각 사례마다 **정답 레이블(= 구역의 주택 가격 중앙값)** 이 주어지므로, 전형적인 **지도학습** 문제입니다.

* **분류인가, 회귀인가, 다른 것인가?**

  * **값을 예측**해야 하므로 전형적인 **회귀** 문제입니다.
  * 더 구체적으로, 여러 특성으로 예측하므로 **다중 회귀(multiple regression)** 입니다.
  * 또한 각 구역마다 **하나의 값만** 예측하므로 **단변량 회귀(univariate regression)** 입니다. 만약 구역당 여러 값을 예측한다면 **다변량 회귀(multivariate regression)** 가 됩니다.

* **배치 학습을 쓸까, 온라인 학습을 쓸까?**

  * 시스템에 데이터가 **연속적으로 들어오지 않고**, **급격한 데이터 변화에 즉각 적응할 필요도** 없으며, 데이터가 **메모리에 담길 만큼 작기** 때문에 **일반적인 배치 학습**이면 충분합니다.

### Select a Performance Measure

* 회귀 문제에서 흔히 쓰는 성능 지표는 **RMSE(평균제곱근오차)** 입니다.

$$
\mathrm{RMSE}(\mathbf{X}, h)=\sqrt{\frac{1}{m}\sum_{i=1}^{m}\bigl(h(\mathbf{x}^{(i)})-y^{(i)}\bigr)^2}
$$

* $m$: 데이터셋의 샘플 개수
* $\mathbf{x}^{(i)}$: i번째 샘플의 **특성 벡터**(라벨 제외)
* $y^{(i)}$: i번째 샘플의 **라벨**(원하는 출력 값)
* $\mathbf{X}$: 모든 샘플의 특성 값을 담은 **행렬**(라벨 제외). 샘플 하나당 한 행이며, i번째 행은 $(\mathbf{x}^{(i)})^\top$
* $h$: 시스템의 **예측 함수**(가설, hypothesis)

  * (\hat{y}^{(i)} = h(\mathbf{x}^{(i)}))
* $\mathrm{RMSE}(\mathbf{X}, h)$: 가설 (h)를 사용해 주어진 예제 집합에서 계산한 **비용 함수 값**(오차 척도)

* 회귀 문제에서 일반적으로는 **RMSE**가 선호되지만, 상황에 따라 다른 함수를 쓰는 것이 더 적절할 수 있습니다.

  * 예: **이상치(outlier)** 구역이 많다고 가정해 봅시다.
  * 이런 경우에는 **MAE(평균절대오차, Average Absolute Deviation)** 사용을 고려할 수 있습니다.

**식 2-2. MAE(Mean Absolute Error)**

$$
\mathrm{MAE}(\mathbf{X}, h)=\frac{1}{m}\sum_{i=1}^{m}\left|,h(\mathbf{x}^{(i)})-y^{(i)},\right|
$$

* **RMSE와 MAE**는 두 벡터 사이의 거리를 재는 방법이며, 여러 **노름(norm)** 이 가능합니다.

  * **제곱합의 제곱근**을 구하는 RMSE는 **유클리드 노름**, 즉 (\ell_2)-노름에 해당합니다.
  * **절댓값의 합**을 구하는 MAE는 **(\ell_1)-노름**에 해당하며, **맨해튼 노름(Manhattan norm)** 이라고도 합니다.

* **노름의 지수가 클수록** 큰 오차에 더 집중하고 작은 오차는 무시하는 경향이 있습니다.

  * 그래서 **RMSE가 MAE보다 이상치(outlier)에 더 민감**합니다.
  * 그러나 이상치가 종 모양 분포처럼 **매우 드문 경우**에는 **RMSE가 성능이 좋고 일반적으로 선호**됩니다.

### Check the Assumptions

* 마지막으로, 지금까지 세운 **가정들을 나열하고 검증**하는 습관이 중요합니다.

  * 예: **다운스트림 시스템**이 가격을 실제 값 대신 **범주(“저렴”, “보통”, “비쌈”)** 로 변환해 사용한다면?
  * 이 경우 **정확한 가격을 완벽히 맞추는 것**은 전혀 중요하지 않습니다.
  * 그러면 이 문제는 **회귀가 아니라 분류 문제**로 정의했어야 합니다.
  * **몇 달 동안 회귀 시스템을 만든 뒤**에야 이 사실을 깨닫는 일은 피해야 합니다.

---

## 2. Get the Data

### Before We Start

* 실습에는 **Google Colab**을 사용합니다.

  * 계정을 만들고 Google Colab에 접속하세요.

* 교과서용 **GitHub/Colab** 리포지토리가 있습니다.

  * **2판(2nd edition)**
    • [https://github.com/ageron/handson-ml2](https://github.com/ageron/handson-ml2)
  * **3판(3rd edition)**
    • [https://homl.info/colab3](https://homl.info/colab3)  
    • 각 장(챕터)당 하나씩, 주피터 노트북 목록을 보여줍니다.


### The Power and Danger of Interactivity

* **주피터 노트북은 상호작용적**입니다.

  * 셀을 하나씩 실행하고, 아무 지점에서 멈추고, 셀을 삽입하고, 코드를 가지고 놀고, 뒤로 가서 같은 셀을 다시 실행하는 등이 가능합니다.

* 하지만 이 **유연성에는 대가**가 있습니다.

  * 셀을 **잘못된 순서**로 실행하기가 매우 쉽습니다.
  * 혹은 **셀 실행을 깜박**하기 쉽습니다.
  * 이렇게 되면 그 다음 **이후 셀들이 실패**할 가능성이 큽니다.


### Download the Data

* 일반적인 환경에서는 데이터가 관계형 데이터베이스(또는 다른 데이터 저장소)에 있고, 여러 테이블/문서/파일에 걸쳐 분산되어 있습니다.

* 하지만 이 프로젝트에서는 훨씬 단순합니다. **housing.tgz** 라는 하나의 압축 파일만 다운로드하면 되며, 그 안에 모든 데이터가 담긴 **housing.csv**(CSV 형식) 파일이 들어 있습니다.

* 최신 데이터를 가져오기 위해 이 함수를 사용하는 **간단한 스크립트**를 작성할 수 있습니다.

* **데이터를 가져오는 함수**

```python
import os
import tarfile
import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
```

* **pandas로 데이터 적재하기**

```python
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
```

### Take a Quick Look at the Data Structure

* DataFrame의 `head()` 메서드를 사용해 **상위 5개 행**을 확인하세요.
```python
housing = load_housing_data()
housing.head()
```
* 각 행은 **하나의 구역(district)** 을 나타내고, 전체 **특성은 10개**입니다:
  **longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity**.

* `info()` 메서드는 데이터의 **요약 정보**를 빠르게 확인할 때 유용합니다.
  특히 **전체 행 수**, **각 특성의 자료형**, **널이 아닌 값의 개수**를 보여줍니다.
```python
housing.info()
```
  * 이 데이터셋에는 **20,640개**의 샘플이 있습니다.
  * **`total_bedrooms`** 특성은 **널이 아닌 값이 20,433개**뿐이므로, **207개 구역**에서 이 특성이 **누락**되어 있습니다.
  * **모든 특성은 수치형**이며, **`ocean_proximity`**만 예외입니다. 이 필드의 자료형은 **object**인데, CSV에서 로드했으므로 **문자열(범주형 텍스트)** 특성임을 알 수 있습니다.

* 상위 5개 행을 볼 때 `ocean_proximity` 열의 값들이 반복적이라는 걸 눈치챘을 겁니다. 이는 이 열이 **범주형(categorical)** 특성임을 의미합니다.
* `value_counts()` 메서드를 사용하면 **존재하는 범주**와 **각 범주에 속한 구역 수**를 확인할 수 있습니다:

```python
housing["ocean_proximity"].value_counts()
```
```
<1H OCEAN    9136
INLAND       6551
NEAR OCEAN   2658
NEAR BAY     2290
ISLAND          5
Name: ocean_proximity, dtype: int64
```

* `describe()` 메서드는 **수치형 특성의 요약 통계**를 보여줍니다.
```python
housing.describe()
```

* 표의 **25% / 50% / 75%** 행은 각각 해당 **백분위수(percentile)** 를 의미합니다.
  백분위수는 관측값들을 정렬했을 때, **해당 비율 이하에 놓이는 값**을 나타냅니다.

  * 예를 들어, 구역의 **`housing_median_age`** 에 대해
    **25%**의 구역은 값이 **18 미만**,
    **50%(중앙값)**의 구역은 **29 미만**,
    **75%**의 구역은 **37 미만**입니다.

* 다루는 데이터의 성격을 빠르게 파악하는 또 다른 방법은 **각 수치형 특성별로 히스토그램을 그려보는 것**입니다.

예시 코드(주피터 노트북 기준):

```python
%matplotlib inline  # 주피터에서만 사용
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20, 15))
plt.show()
```

* 위 코드는 `housing` 데이터프레임의 **모든 수치형 열**에 대해 **빈도 분포(히스토그램)** 를 그려, 값의 분포·치우침(왜도)·이상치 존재 여부 등을 한눈에 파악하게 해줍니다.

* 히스토그램에서 **놓치기 쉬운 몇 가지 점**:

  * 먼저, **`median_income`(소득 중앙값)** 은 **달러(USD)** 로 직접 표기된 것이 아닙니다. 이 값은 **스케일 조정**이 되었고, **상한 15**, **하한 0.5** 로 **절단(capping)** 처리되었습니다. 숫자는 **대략 “만 달러 단위”** 를 뜻합니다(예: 3 ≈ **$30,000**).
  * **`housing_median_age`** 와 **`median_house_value`** 도 **상한 절단**이 있습니다. 특히 **`median_house_value`** 는 **목표 변수(레이블)** 이므로 문제가 될 수 있습니다. **$500,000** 를 넘는 구간까지 **정밀한 예측**이 필요하다면 다음 두 가지 선택지가 있습니다:
    • **절단된 구역**에 대해 **정확한 레이블을 추가로 수집**한다.
    • 해당 구역들을 **학습/테스트 세트에서 제거**한다.
  * 특성들 간에 **스케일(값의 범위)이 크게 다릅니다**.
  * 마지막으로, 많은 히스토그램이 **꼬리가 두터운(tail-heavy)** 형태입니다. 이는 일부 ML 알고리즘이 **패턴을 찾기 어렵게** 만들 수 있습니다. 이후에 **분포를 종 모양(정규형)에 가깝게** 만들도록 **변환(트랜스폼)** 을 시도할 것입니다.


### Create a Test Set

* 우리의 뇌는 **패턴 탐지**에 탁월하지만, 그만큼 **과대적합(overfitting)** 에 취약합니다. 테스트 세트를 미리 들여다보면 테스트 데이터에서 보이는 “그럴듯한” 패턴 때문에 특정 종류의 ML 모델을 선택하게 될 수 있습니다.

  * 이렇게 되면 테스트 세트로 추정한 **일반화 오차**가 **너무 낙관적**이 되어, 실제 성능이 기대에 못 미치는 시스템을 배포하게 됩니다. 이를 **데이터 스누핑(data snooping) 편향**이라 합니다.

* **테스트 세트 생성**

  * 데이터셋에서 무작위로 일부 샘플을 떼어 **보통 20%** 정도(데이터가 매우 크면 더 적게) 분리해 둡니다.

```python
import numpy as np

def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```

* 코드는 동작하지만 **완벽하진 않습니다**: 프로그램을 다시 실행하면 **다른 테스트 세트**가 생성됩니다.

  * 시간이 지나면 당신(또는 ML 알고리즘)이 **전체 데이터셋을 모두 보게 되어**, 피해야 할 **정보 누출**이 발생할 수 있습니다.

* **해결책**

  * 첫 실행 때 만든 **테스트 세트를 저장**해 두고 이후 실행에서는 그것을 로드합니다.
  * `np.random.permutation()`을 호출하기 전에 **난수 시드**를 설정하여 **항상 동일한 셔플 인덱스**가 나오게 합니다.
  * 각 샘플에 **유일하고 변하지 않는 식별자**가 있다고 가정하고, 그 식별자를 사용해 **테스트 세트 포함 여부를 결정**합니다.
 

```python
from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```

* 안타깝게도 **housing 데이터셋에는 고유 식별자 컬럼이 없습니다.**
  가장 간단한 해결책은 **행 인덱스(row index)** 를 ID로 쓰는 것입니다.

```python
housing_with_id = housing.reset_index()   # 'index' 컬럼이 추가됨
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")
```

* 행 인덱스를 고유 식별자로 사용할 경우,

  * **새 데이터는 항상 데이터셋의 끝에만 추가**되고
  * **어떤 행도 삭제되지 않도록** 보장해야 합니다. (그렇지 않으면 ID가 바뀔 수 있음)

* 이것이 어렵다면, **가장 안정적인 특성**을 조합해 고유 ID를 만들 수 있습니다. (예: 구역의 위도·경도)

```python
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")
```

* 지금까지는 **완전 무작위 추출**만 고려했습니다.

  * 데이터셋이 충분히 크다면(특히 **특성 수에 비해** 크다면) 대체로 괜찮지만, 그렇지 않다면 **표본 추출 편향**이 생길 위험이 큽니다.
  * 예: 미국 인구는 여성 51.3%, 남성 48.7%입니다. 잘 설계된 설문이라면 표본 1,000명 중 **여성 513명, 남성 487명**처럼 이 비율을 유지하려고 합니다.
  * 이를 **층화 추출(stratified sampling)** 이라고 합니다. 전체 집단을 **동질적인 하위 집단(층, strata)** 으로 나눈 뒤, 각 층에서 **적절한 수의 표본**을 뽑아 전체 모집단을 대표하도록 **테스트 세트의 대표성**을 보장합니다.


* 전문가들과 대화해 보니 **소득 중앙값(median income)** 이 **주택 가격 중앙값 예측에 매우 중요한 특성**이라고 합니다.

  * 소득 중앙값은 **연속형 수치 특성**이므로, 먼저 **소득을 범주형으로 구분한 특성**을 만들어야 합니다.
  * 각 **층(stratum)** 마다 **충분한 샘플 수**가 데이터셋에 있어야 합니다. 그렇지 않으면 해당 층의 중요도 추정이 **편향**될 수 있습니다.

예시 코드:

```python
housing["income_cat"] = pd.cut(
    housing["median_income"],
    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
    labels=[1, 2, 3, 4, 5]
)

housing["income_cat"].hist()
```

* **소득 카테고리**를 기준으로 **층화 추출**을 수행합니다.

```python
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set  = housing.loc[test_index]
```

* 테스트 세트에서 **소득 카테고리 비율**을 확인합니다.

```python
strat_test_set["income_cat"].value_counts() / len(strat_test_set)
```

출력 예:

```
3    0.350533
2    0.318798
4    0.176357
5    0.114583
1    0.039729
Name: income_cat, dtype: float64
```

* **전체 데이터셋에서의 소득 카테고리 비율**
  (표: Overall / Stratified / Random / Rand. %error / Strat. %error 비교)

* 이제 데이터셋을 **원래 상태로 되돌리기 위해** `income_cat` 특성을 제거합니다:

```python
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

---

## 3. Discover and visualize the data to gain insights

* 먼저 **테스트 세트는 따로 떼어두고**, **훈련 세트만** 탐색하고 있는지 확인하세요.
* 훈련 세트가 매우 큰 경우, 조작을 쉽고 빠르게 하기 위해 **탐색용 샘플 세트**를 따로 뽑아 사용할 수 있습니다.
* 지리 정보를 시각화해 봅시다.

```python
housing = strat_train_set.copy()

housing.plot(kind="scatter", x="longitude", y="latitude")
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
```

* **주택 가격을 살펴봅시다.**

```python
housing.plot(
    kind="scatter", x="longitude", y="latitude", alpha=0.4,
    s=housing["population"]/100, label="population", figsize=(10, 7),
    c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True,
)
plt.legend()
```

(그림: 경도–위도 산점도. 점의 **크기**는 인구 `population`, **색상**은 주택 가격 중앙값 `median_house_value`, 우측 컬러바 표시)

### Looking for Correlations

* 이 데이터셋은 그리 크지 않으므로, **모든 특성 쌍 사이의 표준 상관계수(피어슨 r)** 를 쉽게 계산할 수 있습니다.

  * 단, **상관계수는 선형 상관만** 측정합니다.

```python
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```

(예시 출력: `median_house_value` 와의 상관이 큰 순서로 `median_income` 등이 표시됨. 오른쪽 그림은 다양한 상관 수준·형태의 산점도 예시)

* **모든 수치형 특성**을 **다른 모든 수치형 특성**과 서로 짝지어 그래프로 그립니다.

```python
from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms",
              "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
```

(도표: 각 특성 쌍의 산점도와 대각선의 히스토그램. 빨간 박스는 `median_income`과 `median_house_value` 사이 등 **상관이 비교적 뚜렷한** 조합을 강조)

### Experimenting with Attribute Combinations

* 머신러닝 알고리즘에 데이터를 준비하기 전에, **여러 특성 조합을 시도**해 보는 것도 좋습니다.

```python
housing["rooms_per_household"]   = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_room"]     = housing["total_bedrooms"] / housing["total_rooms"]
housing["population_per_household"] = housing["population"] / housing["households"]

corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```

(예시 출력 요지)

* `median_income`가 목표값과 가장 강한 양의 상관.
* 파생 특성 중 **rooms_per_household**는 약한 양의 상관.
* 원래 특성 **total_rooms**, **total_bedrooms**는 상관이 크지 않음.
* **bedrooms_per_room**는 **음의 상관**을 보여 유의미할 수 있음.

---

## 4. Prepare the Data for Machine Learning Algorithms

### Data Cleaning

* predictors 와 labels 를 분리한다
  
```python
# predictors: `median_house_value` 제외한 나머지 값들
housing = start_train_set.drop("median_house_value", axis=1)

# labels: predictors로 예측할 값, `median_house_value`
housing_labels = start_train_set["median_house_value"].copy()
```

* Take care of missing features

  * Ex: `total_bedrooms` attribute has some missing values, so let’s fix this.

```python
# 1. missing 값이 존재하는 행 (구역) 삭제
housing.dropna(subset=["total_bedrooms"])

# 2. 해당 열 (특성) 삭제
housing.drop("total_bedrooms", axis=1)

# 3. 특정 값(0, 평균, 중앙값 등)으로 채우기
median = housing(["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inpalce=True)
```

* `Scikit-Learn` provides a handy class to take care of missing values: `SimpleImputer`
```python
from sklearn.impute import SimpleImputer

# strategy = "mean", "most_frequent", "constant"
imputer = SimpleImputer(strategy="median")
housing_num = housing.drop("ocean_proximity, axis=1)
imputer.fit(housing_num)
```
* Other sklearn.impute package (both for numerical features only)
  * KNNImputer
    * Replaces each missing value with the mean of the k-nearest neighbor’s values for that feature
  * IterativeImputer
    * Trains a regression model per feature to predict the missing values based on all the other avaiable features
   
### Handling Text and Categorical Attributes

* In this dataset, there is just one text attribute: `the ocean_proximity`
  - It’s not arbitrary text: there are a limited number of possible values, each of which represents a category.
  - So this attribute is a **categorical** attribute.
 
* Let’s convert these categories from text to numbers
  
* One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values.
  - Sol: to create one binary attribute per category
  - This is called **one-hot encoding**.
 
* Handling Text and Categorical Attributes
  * Sparse matrix: After one-hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row.
  * Using up tons of memory mostly to store zeros would be very wasteful (ex: thousands of categories), so instead a sparse matrix only stores the location of the nonzero elements.
 
### Feature Scaling and Transformation

* One of the most important transformations you need to apply to your data is feature scaling.

* Two common ways to get all attributes to have the same scale:
  * min-max scaling (=normalization)
  * standardization
 
### Feature Scaling

* Min-max scaling (=normalization): 정규화
  * Values are shifted and rescaled so that they end up ranging from 0 to 1.

* Standardization: 표준화
  * 평균 0, 표준편차 1로 맞춘다
  * 값을 0-1로 제한하지 않기 때문에 어떤 알고리즘에서는 문제가 될 수 있다
  * outlier의 영향을 적게 받는다
    
* As with all the transformations, it is important to fit the scalers to the **training data only**, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data).

* When a feature’s distribution has a **heavy tail** (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will **squash** most values into a **small range**.
  * Machine learning models generally **don’t like** this at all
 
* So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to **make the distribution roughly symmetrical**.
  * A common way to do this for positive features with a heavy tail to the right is to replace the feature with its **square root** (or raise the feature to a **power** between 0 and 1)
  * If the feature has a really long and heavy tail, then replacing the feature with its **log** may help

* Another approach to handle heavy-tailed features consists in bucketizing the feature
  * Chopping its distribution into roughly equal-sized buckets and replacing each feature value with the index of the bucket it belongs to
 
* When a feature has a multimodal distribution (i.e., with two or more clear peaks, called modes)
  * It can also be helpful to bucketize it, but this time treating the bucket IDs as categories, rather than as numerical values. This means that the bucket indices must be encoded, for example using a OneHotEncoder.
 
### Custom Transformers

* **학습이 필요 없는 변환**이라면, **NumPy 배열을 입력받아 변환된 배열을 출력하는 함수**만 작성해도 됩니다.

* 예시 1: **로그 변환기**를 만들고 적용하기

```python
from sklearn.preprocessing import FunctionTransformer
import numpy as np

log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
log_pop = log_transformer.transform(housing[["population"]])
```

* 예시 2: **가우시안 RBF 유사도** 계산

```python
# rbf_kernel은 (x, y, gamma=...) 형태의 커스텀 함수라고 가정
rbf_transformer = FunctionTransformer(rbf_kernel, kw_args=dict(Y=[[35.]], gamma=0.1))
age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])
```

* 예시 3: **특성 결합(비율 만들기)**

```python
ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
# 결과: array([[0.5],
#             [0.75]])
```

* `FunctionTransformer`는 매우 유용하지만, **`fit()`에서 일부 파라미터를 학습하는 “학습 가능한(transformer)”** 형태가 필요하다면?

  * 이 경우 **커스텀 클래스를 직접 작성**해야 합니다.
  * Scikit-Learn은 **상속보다 덕 타이핑**에 의존합니다. 즉, 클래스 하나를 만들고 **`fit()`(자기 자신 반환), `transform()`, `fit_transform()`** 세 메서드만 구현하면 됩니다.
  * `TransformerMixin`을 베이스 클래스로 추가하면 **`fit_transform()`을 공짜로** 얻을 수 있습니다.
  * 추가로 `BaseEstimator`를 베이스 클래스로 넣고(생성자에서 `*args`, `**kwargs`는 피하고) 만들면, **`get_params()`와 `set_params()`** 두 메서드가 생겨 **자동 하이퍼파라미터 튜닝**에 유용합니다.

# 사용자 정의 변환기

* 예: **직접 StandardScaler 구현하기**

```python
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):   # *args, **kwargs 사용하지 않음
        self.with_mean = with_mean

    def fit(self, X, y=None):             # y 인수는 관례상 필요(사용하지 않아도 됨)
        X = check_array(X)                # 유효한 실수 배열인지 확인
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # 모든 추정기는 fit에 이 속성을 저장
        return self                       # 항상 self 반환

    def transform(self, X):
        check_is_fitted(self)             # 학습된 속성(끝에 _)이 있는지 확인
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
```

요점

* `BaseEstimator`, `TransformerMixin`을 상속해 **`fit`/`transform`** 규약만 맞추면 Scikit-Learn 파이프라인에서 그대로 쓸 수 있습니다.
* `fit()`에서 **평균과 표준편차를 학습**하고, `transform()`에서 ((X-\mu)/\sigma)로 **표준화**합니다.
* `check_array`, `check_is_fitted`으로 입력 유효성·학습 여부를 검증합니다.


### Transformation Pipelines

* Scikit-Learn은 여러 변환을 순차적으로 적용할 수 있도록 **`Pipeline` 클래스**를 제공합니다.

```python
from sklearn.pipeline import Pipeline

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),  # 결측값을 중앙값으로 채우기
    ("standardize", StandardScaler()),             # 표준화
])

housing_num_prepared = num_pipeline.fit_transform(housing_num)
housing_num_prepared[:2].round(2)
# array([[ -1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],
#        [  0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17]])
```

> 요점: 파이프라인을 쓰면 **훈련 세트에 `fit`한 동일한 전처리 단계들을** 테스트/새 데이터에도 **일관되게** 적용할 수 있어요.

* 지금까지는 **범주형 컬럼**과 **수치형 컬럼**을 따로 처리했습니다.

  * 각 컬럼에 알맞은 변환을 적용하되, **모든 컬럼을 한 번에 처리**해 주는 단일 변환기가 있으면 더 편합니다.
  * 이를 위해 Scikit-Learn 0.20부터 **`ColumnTransformer`** 가 도입되었습니다.

```python
from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)          # 수치형 컬럼들
cat_attribs = ["ocean_proximity"]        # 범주형 컬럼

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),  # 수치형: 결측치 처리 + 표준화 등
    ("cat", OneHotEncoder(), cat_attribs) # 범주형: 원-핫 인코딩
])

housing_prepared = full_pipeline.fit_transform(housing)
```

* 요점: `ColumnTransformer`를 쓰면 **컬럼별로 다른 전처리**를 한 파이프라인에서 **일관되게** 적용할 수 있습니다.

---

## 5. Select and Train a Model

### Training and Evaluating on the Training Set

* **선형회귀 모델 학습**

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)
print("Predictions:", lin_reg.predict(some_data_prepared))
print("Labels:", list(some_labels))
```

(예: 예측값과 실제 레이블을 나란히 출력)

* **훈련 세트 전체에 대한 RMSE 측정**

```python
from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse
```

* 출력된 RMSE ≈ **68,628** ⇒ **언더피팅(underfitting)** 징후.

* 더 **복잡한 모델**인 **의사결정나무 회귀(DecisionTreeRegressor)** 를 시도해 봅시다.

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)

housing_predictions = tree_reg.predict(housing_prepared)
tree_mse  = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
tree_rmse   # 예: 0.0  → 과적합(오버피팅) 의심
```

* **테스트 세트는 모델을 최종 확신할 때까지 건드리지 않는 것**이 중요합니다.
  따라서 훈련 세트 일부는 **학습용**, 나머지 일부는 **검증용(모델 검증)** 으로 사용하세요.

### Better Evaluation Using Cross-Validation

* **K-겹 교차검증(K-fold cross-validation)**

  ```python
  from sklearn.model_selection import cross_val_score
  scores = cross_val_score(
      tree_reg, housing_prepared, housing_labels,
      scoring="neg_mean_squared_error", cv=10
  )
  tree_rmse_scores = np.sqrt(-scores)
  ```

  ```python
  def display_scores(scores):
      print("Scores:", scores)
      print("Mean:", scores.mean())
      print("Standard deviation:", scores.std())
  display_scores(tree_rmse_scores)
  ```

  * 출력 예: 평균 RMSE ≈ **71,408**, 표준편차 ≈ **2,439** → **과적합(오버피팅)** 징후.

* **선형회귀 모델에도 동일한 평가 수행**

  ```python
  lin_scores = cross_val_score(
      lin_reg, housing_prepared, housing_labels,
      scoring="neg_mean_squared_error", cv=10
  )
  lin_rmse_scores = np.sqrt(-lin_scores)
  display_scores(lin_rmse_scores)
  ```

  * 출력 예: 평균 RMSE ≈ **69,052**, 표준편차 ≈ **2,731**.

* 마지막으로 **랜덤 포레스트 회귀(RandomForestRegressor)** 를 시도합니다.

```python
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)

# ... (교차검증 코드 생략)
forest_rmse          # 예: 18,603 (훈련 세트에서의 RMSE)
display_scores(forest_rmse_scores)
# Scores: [49519.8, 47461.9, 50029.0, 52325.3, 49308.4, 53446.4, 48634.8, 47585.7, 53490.1, 50021.6]
# Mean: 50182.3
# Standard deviation: 2097.1
```

* **랜덤 포레스트는 유망**해 보입니다.
* 다만 **훈련 세트 점수**가 **검증 세트 점수보다 훨씬 낮다**는 점을 주목하세요. 이는 모델이 **여전히 훈련 세트에 과적합**하고 있음을 의미합니다.

---

## 6. Fine-Tune Your Model

### Grid Search

* 하이퍼파라미터를 손으로 이리저리 바꿔 보며 좋은 조합을 찾을 수도 있지만, **그리드 서치**로 체계적으로 탐색할 수 있습니다.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]

forest_reg = RandomForestRegressor()

grid_search = GridSearchCV(
    forest_reg, param_grid, cv=5,
    scoring='neg_mean_squared_error',
    return_train_score=True
)

grid_search.fit(housing_prepared, housing_labels)
```

* 위 설정에서 그리드 서치는 **3×4 + 2×3 = 18가지** 랜덤 포레스트 하이퍼파라미터 조합을 탐색하고, **5-겹 교차검증**을 쓰므로 **각 조합을 5번씩** 학습합니다.
* 따라서 총 **18 × 5 = 90회**의 학습이 진행됩니다.

```python
grid_search.best_params_
# {'max_features': 8, 'n_estimators': 30}

grid_search.best_estimator_
# RandomForestRegressor( ... max_features=8, ... n_estimators=30, ...)

cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)
```

* `best_params_`: 교차검증 평균 성능이 가장 좋았던 **하이퍼파라미터 조합**
  → 여기서는 `max_features=8`, `n_estimators=30`.

* `best_estimator_`: 위 최적 하이퍼파라미터로 **학습된 모델 객체**.

* `cv_results_`: 각 조합의 교차검증 결과가 담긴 딕셔너리.
  예시 출력의 앞 숫자는 `np.sqrt(-mean_score)`로 계산한 **RMSE**이고, 뒤의 `{...}`는 해당 조합의 하이퍼파라미터입니다.
  이를 보면 조합마다 RMSE가 어떻게 달라지는지 비교할 수 있습니다.


### Randomized Search

* **그리드 서치**는 조합 수가 비교적 적을 때는 괜찮지만, **하이퍼파라미터 탐색 공간이 클 때**는 보통 **`RandomizedSearchCV`** 를 쓰는 것이 더 좋습니다.

* 모든 조합을 전부 시도하는 대신, **각 반복마다 하이퍼파라미터 값을 무작위로 뽑아** 정해진 **무작위 조합 수**만 평가합니다.

* 주요 장점 두 가지:

  * 예를 들어 **1,000번** 돌리면, 그리드 서치처럼 각 파라미터에 대해 몇 개 값만 보는 것이 아니라 **각 파라미터의 1,000가지 서로 다른 값**을 탐색할 수 있습니다.
  * **반복 횟수만 설정**하면 하이퍼파라미터 탐색에 쓸 **연산 예산을 더 쉽게 제어**할 수 있습니다.


### Ensemble Methods

* 모델을 미세 조정하는 또 다른 방법은 **성능이 좋은 여러 모델을 결합**해 보는 것입니다.
* 이렇게 만든 **그룹(= 앙상블)** 은 종종 **개별 모델 중 최고 성능보다 더 좋은 성능**을 냅니다(의사결정나무 여러 개를 모은 **랜덤 포레스트**가 단일 트리보다 보통 더 좋은 것처럼). 특히 **각 모델이 서로 다른 종류의 오류**를 낼 때 효과가 큽니다.


### Analyze the Best Models and Their Errors

* **RandomForestRegressor** 는 **정확한 예측에 각 특성이 기여하는 상대적 중요도(feature importance)** 를 알려줄 수 있습니다.

```python
feature_importances = grid_search.best_estimator_.feature_importances_

extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
cat_encoder = full_pipeline.named_transformers_["cat"]
cat_one_hot_attribs = list(cat_encoder.categories_[0])

attributes = num_attribs + extra_attribs + cat_one_hot_attribs

sorted(zip(feature_importances, attributes), reverse=True)
```

* 예시 결과(상위 → 하위):

  * `median_income`가 가장 중요,
  * 그 다음은 `INLAND`(내륙 여부), `pop_per_hhold`(가구당 인구), `longitude`, `latitude`, `rooms_per_hhold`, `bedrooms_per_room` …
  * 범주형 `ocean_proximity`의 원-핫 항목들(`<1H OCEAN`, `NEAR OCEAN`, `NEAR BAY`, `ISLAND`)도 중요도에 포함됩니다.


### Evaluate Your System on the Test Set

* 이제 **최종 모델을 테스트 세트로 평가**할 차례입니다.

  * 테스트 세트에서 **입력 특성(X)** 과 **레이블(y)** 을 분리합니다.
  * 학습 때 만든 **`full_pipeline`으로 변환만** 수행합니다(**`fit_transform()`이 아니라 `transform()`** — 테스트 세트에 `fit`하면 안 됨).
  * 최종 모델로 **예측**하고, **오차(RMSE)** 를 계산합니다.

```python
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)      # transform만!

final_predictions = final_model.predict(X_test_prepared)

final_mse  = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)    # 예: 47,730.2
```

* **일반화 오차의 95% 신뢰구간**을 계산할 수 있습니다.

  ```python
  from scipy import stats
  confidence = 0.95
  squared_errors = (final_predictions - y_test) ** 2
  np.sqrt(stats.t.interval(
      confidence, len(squared_errors) - 1,
      loc=squared_errors.mean(),
      scale=stats.sem(squared_errors)))
  # 예시 출력: array([45685.10..., 49691.25...])
  ```

  → RMSE의 95% 신뢰구간이 대략 **45,685 ~ 49,691** 이라는 뜻입니다.

* **하이퍼파라미터 튜닝을 많이 했을수록**, 테스트 성능이 교차검증에서 측정했던 것보다 **조금 더 나빠지는** 경우가 흔합니다.
  (검증 세트에 **과도하게 맞춰진** 결과, 새로운 데이터에 덜 일반화될 수 있기 때문.)

* 이런 상황에서 **테스트 세트 점수를 좋게 보이게 하려고** 다시 하이퍼파라미터를 만지려는 유혹을 **참아야** 합니다.
  그렇게 얻은 개선은 **새로운 데이터에는 일반화되지 않을** 가능성이 큽니다.

---

## 7. Launch, Monitor, and Maintain Your System

### Deploy

* **프로덕션 환경에 모델 배포**

  * 학습된 Scikit-Learn 모델(예: `joblib`으로 저장)을 **전처리와 예측 파이프라인 전체 포함**하여 저장한다.
  * 이 저장된 모델을 프로덕션 환경에서 로드한다.
  * 모델의 `predict()` 메서드를 호출해 예측에 사용한다.

* **클라우드에 모델 배포(예: Google Cloud AI Platform)**

  * 모델을 `joblib`으로 저장하고 **Google Cloud Storage(GCS)** 에 업로드한다.
  * **Google Cloud AI Platform**으로 가서 새 **모델 버전**을 만들고, 그 위치를 GCS 파일로 지정한다.
  * 장점: **부하 분산과 스케일링**을 자동으로 처리해 주는 **간단한 웹 서비스** 형태로 사용할 수 있다.


### Monitoring

* **실시간 성능을 정기적으로 확인하고 성능 하락 시 알림을 보내는** 모니터링 코드를 꼭 작성해야 합니다.

  * 성능이 **서서히 감소**할 수도 있는데, 오랫동안 눈치채지 못하기 쉽습니다. 모델은 시간이 지나며 **성능이 ‘썩는(rot)’** 경향이 있어 흔한 일입니다.

* 데이터가 **계속 변화**한다면, **데이터셋을 업데이트**하고 **정기적으로 재학습**해야 합니다.

* 모델의 **입력 데이터 품질**도 점검하세요. 때로는 **신호 품질이 나빠서** 성능이 조금씩 떨어지기도 합니다.

* 마지막으로, 만든 **모든 모델의 백업**을 보관하고, 문제가 생겨 새 모델 성능이 크게 떨어질 때를 대비해 **이전 모델로 빠르게 롤백**할 수 있는 프로세스와 도구를 갖추세요.

